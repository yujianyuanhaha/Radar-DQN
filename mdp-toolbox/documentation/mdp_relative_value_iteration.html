<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta name="keywords" content="Markov decision process, Toolbox, MATLAB">
   <title> mdp_relative_value_iteration description </title>
</head>
<body bgcolor="#FFFFFF">

<center><table BORDER=0 CELLSPACING=0 CELLPADDING=0 COLS=2 WIDTH="100%" BGCOLOR="#FFFFCC" NOSAVE >
<tr BGCOLOR="#FFFFCC" NOSAVE>
<td BGCOLOR="#FFFFCC" NOSAVE><b>MDP Toolbox for MATLAB</b></td>
<td><div align=right><a href="DOCUMENTATION.html"><img SRC="arrow.gif" height=23 width=26></a></div></td>
</tr>
</table></center>

<p><b><font color="#FF0000"><font size=+2>mdp_relative_value_iteration</font></font></b>
<p><font color="#000000">Solves MDP with average reward with relative value iteration algorithm.</font>


<p><b><font color="#3366FF"><font size=+1>Syntax</font></font></b>
<font color="#000000">
<p> [policy, average_reward, cpu_time] = mdp_relative_value_iteration (P, R)
<br> [policy, average_reward, cpu_time] = mdp_relative_value_iteration (P, R, epsilon)
<br> [policy, average_reward, cpu_time] = mdp_relative_value_iteration (P, R, epsilon, max_iter)
</font>


<p><b><font color="#3366FF"><font size=+1>Description</font></font></b>
<font color="#000000">
<p>mdp_relative_value_iteration applies the relative value iteration
algorithm to solve MDP with average reward. The algorithm
consists in solving optimality equations iteratively. 
<br>Iterating is stopped when an epsilon-optimal policy is found or after a specified number (max_iter) of iterations is done.
<br>This fonction uses verbose and silent modes. In verbose mode, the function
displays the span of (U<sub>n+1</sub>-U<sub>n</sub>) for each iteration.
</font>


<p><b><font color="#3366FF"><font size=+1>Arguments</font></font></b>
<font color="#000000">
<ul><li><b>P : </b>transition probability array.</li></ul>
P can be a 3 dimensions array (SxSxA) or a cell array (1xA), each cell containing a sparse matrix (SxS).
<ul><li><b>R : </b>reward array.</li></ul>
R can be a 3 dimensions array (SxSxA) or a cell array (1xA), each cell containing a sparse matrix (SxS) or a 2D array (SxA) possibly sparse.
<ul><li><b>epsilon (optional) : </b>search for an epsilon-optimal policy.</li></ul>
epsilon is a real in [0; 1].
<br> By default, epsilon is set to 0.01.
<ul><li><b>max_iter (optional) : </b>maximum number of iterations.</li></ul>
max_iter is an integer greater than 0.
<br> By default, max_iter is set to 1000.
</font>


<p><b><font color="#3366FF"><font size=+1>Evaluations</font></font></b>
<font color="#000000">
<ul><li><b>policy : </b>optimal policy.</li></ul>
policy is a (Sx1) vector. Each element is an integer
corresponding to an action which maximizes the value function.
<ul><li><b>average_reward : </b>average reward of the optimal policy.</li></ul>
average_reward is a real.
<ul><li><b>cpu_time : </b>CPU time used to run the program.</li></ul>
</font>


<p><b><font color="#3366FF"><font size=+1>Example</font></font></b><br>
<font color="#999999">In grey, verbose mode display.</font><br>
<p><font color="#000000">
>> P(:,:,1) = [ 0.5 0.5; &nbsp 0.8 0.2 ];<br>
>> P(:,:,2) = [ 0   1; &nbsp 0.1 0.9 ];<br>
>> R = [ 5 10; &nbsp -1 2 ];<br>
<br>
>> [policy, average_reward, cpu_time] = mdp_relative_value_iteration(P, R)<br>
<font color="#999999">&nbsp&nbsp  Iteration  U_variation<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      1     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    8<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      2     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    3.4<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      3     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    2.72<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      4     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    2.176<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      5     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    1.7408<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      6     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    1.3926<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      7     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    1.1141<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      8     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    0.89129<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      9     &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp    0.71303<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      10    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.57043<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      11    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.45634<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      12    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.36507<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      13    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.29206<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      14    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.23365<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      15    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.18692<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      16    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.14953<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      17    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.11963<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      18    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.095701<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      19    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.076561<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      20    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.061249<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      21    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.048999<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      22    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.039199<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      23    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.031359<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      24    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.025088<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      25    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.02007<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      26    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.016056<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      27    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.012845<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      28    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.010276<br>
&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp      29    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp     0.0082207<br>
MDP Toolbox : iterations stopped, epsilon-optimal policy found<br></font>
policy =<br>
&nbsp&nbsp     2<br>
&nbsp&nbsp     1<br>
average_reward =<br>
&nbsp&nbsp    3.8852<br>
cpu_time =<br>
&nbsp&nbsp    0.0800<br>
<br>
In the above example, P can be a cell array containing sparse matrices:<br>
>> P{1} = sparse([ 0.5 0.5;&nbsp   0.8 0.2 ]);<br>
>> P{2} = sparse([ 0 1;&nbsp   0.1 0.9 ]);<br>
The function call is unchanged.<br>
</font>


<br><br>
<table BORDER=0 CELLSPACING=0 CELLPADDING=0 COLS=2 WIDTH="100%" BGCOLOR="#FFFFCC" NOSAVE >
<tr NOSAVE>
<td NOSAVE><b>MDP Toolbox for MATLAB</b></td>
<td><div align=right><a href="DOCUMENTATION.html"><img SRC="arrow.gif" height=23 width=26></a></div></td>
</tr>
</table>

<br><br>
<hr WIDTH="100%">
<font size=-1>MDPtoolbox/documentation/mdp_relative_value_iteration.html
<br>Page created on July 31, 2001. Last update on October 31, 2012.</font>
</body>
</html>
