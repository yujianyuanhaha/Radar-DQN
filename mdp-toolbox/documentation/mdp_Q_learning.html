<!doctype html public "-//w3c//dtd html 4.0 transitional//en">
<html>
<head>
   <meta name="keywords" content="Markov decision process, Toolbox, MATLAB">
   <title> mdp_Q_learning description </title>
</head>
<body bgcolor="#FFFFFF">

<center><table BORDER=0 CELLSPACING=0 CELLPADDING=0 COLS=2 WIDTH="100%" BGCOLOR="#FFFFCC" NOSAVE >
<tr BGCOLOR="#FFFFCC" NOSAVE>
<td BGCOLOR="#FFFFCC" NOSAVE><b>MDP Toolbox for MATLAB</b></td>
<td><div align=right><a href="DOCUMENTATION.html"><img SRC="arrow.gif" height=23 width=26></a></div></td>
</tr>
</table></center>

<p><b><font color="#FF0000"><font size=+2>mdp_Q_learning</font></font></b>
<p><font color="#000000">Solves discounted MDP with the Q-learning algorithm (Reinforcement learning).</font>

<p><b><font color="#3366FF"><font size=+1>Syntax</font></font></b>
<font color="#000000">
<p> [Q, V, policy, mean_discrepancy] = mdp_Q_learning(P, R, discount).<br>
[Q, V, policy, mean_discrepancy] = mdp_Q_learning(P, R, discount, N)<br>
</font>


<p><b><font color="#3366FF"><font size=+1>Description</font></font></b>
<font color="#000000">
<p>mdp_Q_learning computes the Q matrix, the mean discrepancy and gives the optimal value function and the optimal policy when allocated enough iterations.  It uses an iterative method.<br>
No additional display in verbose mode.
</font>

<p><b><font color="#3366FF"><font size=+1>Arguments</font></font></b>
<font color="#000000">
<ul><li><b>P : </b>transition probability array.</li></ul>
P can be a 3 dimensions array (SxSxA) or a cell array (1xA), each cell containing a sparse matrix (SxS).
<ul><li><b>R : </b>reward array.</li></ul>
R can be a 3 dimensions array (SxSxA) or a cell array (1xA), each cell containing a sparse matrix (SxS) or a 2D array (SxA) possibly sparse.
<ul><li><b>discount : </b>discount factor.</li></ul> 
discount is a real which belongs to ]0; 1[
<ul><li><b>N (optional) : </b>number of iterations to perform.</li></ul> 
N is an integer that must be greater than the default value.<br> 
By default, N is set to 10000.
</font>


<p><b><font color="#3366FF"><font size=+1>Evaluations</font></font></b>
<font color="#000000">
<ul><li><b>Q :</b> an action-value function that gives the expected utility of taking a given action in a given state and following an optimal policy thereafter.</li></ul> 
Q is a (SxA) matrix.
<ul><li><b> mean_discrepancy :</b> discrepancy means over 100 iterations.</li></ul> 
mean_discrepancy is a vector of V discrepancy mean over 100 iterations. Then the length of the vector for the default value of N is 100.
<ul><li><b>V :</b> value function.</li></ul> 
V is a (Sx1) vector.
<ul><li><b>policy : </b>policy.</li></ul>
policy is a (Sx1) vector. Each element is an integer
corresponding to an action which maximizes the value function.
</font>

<p><b><font color="#3366FF"><font size=+1>Example</font></font></b>
<p><font color="#000000">
>> % To be able to reproduce the following example, it is necessary to init the pseudorandom number generator<br>
>> rand('seed',0) <br>
<br>
>> P(:,:,1) = [ 0.5 0.5; &nbsp 0.8 0.2 ];<br>
>> P(:,:,2) = [ 0   1; &nbsp 0.1 0.9 ];<br>
>> R = [ 5 10; &nbsp -1 2 ];<br>
<br>
>> [Q, V, policy, mean_discrepancy] = mdp_Q_learning(P, R, 0.9);<br>
>> Q<br>
Q =<br>
&nbsp&nbsp   31.8074 &nbsp&nbsp  39.0360<br>
&nbsp&nbsp   32.7959 &nbsp&nbsp  26.7980<br>
>> V<br>
V =<br>
&nbsp&nbsp   39.0360<br>
&nbsp&nbsp   32.7959<br>
>> policy<br>
policy =<br>
&nbsp&nbsp     2<br>
&nbsp&nbsp     1<br>
>>plot(mean_discrepancy)<br>
<img SRC="meandiscrepancy.jpg">
<br>
In the above example, P can be a cell array containing sparse matrices:<br>
>> P{1} = sparse([ 0.5 0.5;&nbsp   0.8 0.2 ]);<br>
>> P{2} = sparse([ 0 1;&nbsp   0.1 0.9 ]);<br>
The function call is unchanged.<br>
</font>


<br><br> 
<table BORDER=0 CELLSPACING=0 CELLPADDING=0 COLS=2 WIDTH="100%" BGCOLOR="#FFFFCC" NOSAVE >
<tr NOSAVE>
<td NOSAVE><b>MDP Toolbox for MATLAB</b></td>
<td>
<div align=right><a href="DOCUMENTATION.html"><img SRC="arrow.gif" height=23 width=26></a></div>
</td>
</tr>
</table>

<br>
<hr WIDTH="100%">
<font size=-1>File : MDPtoolbox/documentation/mdp_Q_learning.html
<br>Page created on August 31, 2009. </font>
</body>
</html>
